URL: https://arxiv.org/abs/2502.06703

[2502.06703] Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling
Skip to main content
We gratefully acknowledge support from the Simons Foundation,
member institutions
, and all contributors.
Donate
>
cs
>
arXiv:2502.06703
Help
|
Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Computer Science > Computation and Language
arXiv:2502.06703
(cs)
[Submitted on 10 Feb 2025]
Title:
Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling
Authors:
Runze Liu
,
Junqi Gao
,
Jian Zhao
,
Kaiyan Zhang
,
Xiu Li
,
Biqing Qi
,
Wanli Ouyang
,
Bowen Zhou
View a PDF of the paper titled Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling, by Runze Liu and 7 other authors
View PDF
HTML (experimental)
Abstract:
Test-Time Scaling (TTS) is an important method for improving the performance of Large Language Models (LLMs) by using additional computation during the inference phase. However, current studies do not systematically analyze how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS. This lack of analysis limits the understanding and practical use of TTS methods. In this paper, we focus on two core questions: (1) What is the optimal approach to scale test-time computation across different policy models, PRMs, and problem difficulty levels? (2) To what extent can extended computation improve the performance of LLMs on complex tasks, and can smaller language models outperform larger ones through this approach? Through comprehensive experiments on MATH-500 and challenging AIME24 tasks, we have the following observations: (1) The compute-optimal TTS strategy is highly dependent on the choice of policy model, PRM, and problem difficulty. (2) With our compute-optimal TTS strategy, extremely small policy models can outperform larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500. Moreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM surpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher inference efficiency. These findings show the significance of adapting TTS strategies to the specific characteristics of each task and model and indicate that TTS is a promising approach for enhancing the reasoning abilities of LLMs.
Subjects:
Computation and Language (cs.CL)
Cite as:
arXiv:2502.06703
[cs.CL]
(or
arXiv:2502.06703v1
[cs.CL]
for this version)
https://doi.org/10.48550/arXiv.2502.06703
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Runze Liu [
view email
]
[v1]
Mon, 10 Feb 2025 17:30:23 UTC (744 KB)
Full-text links:
Access Paper:
View a PDF of the paper titled Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling, by Runze Liu and 7 other authors
View PDF
HTML (experimental)
TeX Source
Other Formats
view license
Current browse context:
cs.CL
< prev
|
next >
new
|
recent
|
2025-02
Change to browse by:
cs
References & Citations
NASA ADS
Google Scholar
Semantic Scholar
a
export BibTeX citation
Loading...
BibTeX formatted citation
×
loading...
Data provided by:
Bookmark
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer
(
What is the Explorer?
)
Connected Papers Toggle
Connected Papers
(
What is Connected Papers?
)
Litmaps Toggle
Litmaps
(
What is Litmaps?
)
scite.ai Toggle
scite Smart Citations
(
What are Smart Citations?
)
Code, Data, Media
Code, Data and Media Associated with this Article
alphaXiv Toggle
alphaXiv
(
What is alphaXiv?
)
Links to Code Toggle
CatalyzeX Code Finder for Papers
(
What is CatalyzeX?
)
DagsHub Toggle
DagsHub
(
What is DagsHub?
)
GotitPub Toggle
Gotit.pub
(
What is GotitPub?
)
Huggingface Toggle
Hugging Face
(
What is Huggingface?
)
Links to Code Toggle
Papers with Code
(
What is Papers with Code?
)
ScienceCast Toggle
ScienceCast
(
What is ScienceCast?
)
Demos
Demos
Replicate Toggle
Replicate
(
What is Replicate?
)
Spaces Toggle
Hugging Face Spaces
(
What is Spaces?
)
Spaces Toggle
TXYZ.AI
(
What is TXYZ.AI?
)
Related Papers
Recommenders and Search Tools
Link to Influence Flower
Influence Flower
(
What are Influence Flowers?
)
Core recommender toggle
CORE Recommender
(
What is CORE?
)
Author
Venue
Institution
Topic
About arXivLabs
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community?
Learn more about arXivLabs
.
Which authors of this paper are endorsers?
|
Disable MathJax
(
What is MathJax?
)
About
Help
contact arXiv
Click here to contact arXiv
Contact
subscribe to arXiv mailings
Click here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or
slack